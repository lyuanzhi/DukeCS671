{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5d51b54-b66c-4b27-8f52-6ded3de01b29",
   "metadata": {},
   "source": [
    "# Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d868e8ca-f1af-4f15-9580-c222aceb087f",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a350f3-b564-4482-83fe-c1f498bcf565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from category_encoders import BinaryEncoder\n",
    "import ast\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1851488-ac80-4afb-999f-5e96a8f7e17e",
   "metadata": {},
   "source": [
    "## Set Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d75c673-e59a-4551-a79c-5ddc5cfece6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "def set_all_seeds(RANDOM_SEED):\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_all_seeds(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56404f-f883-4f2b-bb31-02915475343e",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e88ecc4-2712-4ce4-8f84-cf7e7a977055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# initial removal of unwanted features\n",
    "drop_columns = ['id','scrape_id','last_scraped','name','description','picture_url','host_id','host_name',\n",
    "                'neighbourhood_cleansed','property_type','calendar_last_scraped']\n",
    "train_data = train_data.drop(drop_columns, axis=1)\n",
    "test_data = test_data.drop(drop_columns, axis=1)\n",
    "\n",
    "# delete incomplete data\n",
    "train_data = train_data.dropna()\n",
    "\n",
    "binary = 0\n",
    "if binary:\n",
    "    # binary encode room_type\n",
    "    binary_encoder = BinaryEncoder(cols=['room_type'])\n",
    "    binary_encoded_data = binary_encoder.fit_transform(train_data['room_type'])\n",
    "    for i in binary_encoded_data.columns:\n",
    "        train_data[i] = binary_encoded_data[i]\n",
    "    train_data = train_data.drop(['room_type'], axis=1)\n",
    "    \n",
    "    # binary encode neighbourhood_group_cleansed\n",
    "    binary_encoder = BinaryEncoder(cols=['neighbourhood_group_cleansed'])\n",
    "    binary_encoded_data = binary_encoder.fit_transform(train_data['neighbourhood_group_cleansed'])\n",
    "    for i in binary_encoded_data.columns:\n",
    "        train_data[i] = binary_encoded_data[i]\n",
    "    train_data = train_data.drop(['neighbourhood_group_cleansed'], axis=1)\n",
    "else:\n",
    "    # label encode room_type\n",
    "    train_data.loc[train_data['room_type'] == 'Shared room', 'room_type'] = 0\n",
    "    train_data.loc[train_data['room_type'] == 'Entire home/apt', 'room_type'] = 1\n",
    "    train_data.loc[train_data['room_type'] == 'Private room', 'room_type'] = 2\n",
    "    train_data.loc[train_data['room_type'] == 'Hotel room', 'room_type'] = 3\n",
    "    train_data['room_type'] = train_data['room_type'].astype('float64')\n",
    "\n",
    "    test_data.loc[test_data['room_type'] == 'Shared room', 'room_type'] = 0\n",
    "    test_data.loc[test_data['room_type'] == 'Entire home/apt', 'room_type'] = 1\n",
    "    test_data.loc[test_data['room_type'] == 'Private room', 'room_type'] = 2\n",
    "    test_data.loc[test_data['room_type'] == 'Hotel room', 'room_type'] = 3\n",
    "    test_data['room_type'] = test_data['room_type'].astype('float64')\n",
    "\n",
    "    # label encode neighbourhood_group_cleansed\n",
    "    train_data.loc[train_data['neighbourhood_group_cleansed'] == 'Unincorporated Areas', 'neighbourhood_group_cleansed'] = 0\n",
    "    train_data.loc[train_data['neighbourhood_group_cleansed'] == 'Other Cities', 'neighbourhood_group_cleansed'] = 1\n",
    "    train_data.loc[train_data['neighbourhood_group_cleansed'] == 'City of Los Angeles', 'neighbourhood_group_cleansed'] = 2\n",
    "    train_data['neighbourhood_group_cleansed'] = train_data['neighbourhood_group_cleansed'].astype('float64')\n",
    "\n",
    "    test_data.loc[test_data['neighbourhood_group_cleansed'] == 'Unincorporated Areas', 'neighbourhood_group_cleansed'] = 0\n",
    "    test_data.loc[test_data['neighbourhood_group_cleansed'] == 'Other Cities', 'neighbourhood_group_cleansed'] = 1\n",
    "    test_data.loc[test_data['neighbourhood_group_cleansed'] == 'City of Los Angeles', 'neighbourhood_group_cleansed'] = 2\n",
    "    test_data['neighbourhood_group_cleansed'] = test_data['neighbourhood_group_cleansed'].astype('float64')\n",
    "\n",
    "# convert host_verifications to onehot\n",
    "# new_cols = ['email', 'phone', 'work_email']\n",
    "# for i in new_cols: \n",
    "#     train_data[i] = 0\n",
    "# for i in train_data.index:\n",
    "#     for item in new_cols:\n",
    "#         if item in ast.literal_eval(train_data.loc[i, 'host_verifications']):\n",
    "#             train_data.loc[i, item] = 1\n",
    "train_data = train_data.drop(['host_verifications'], axis=1)\n",
    "\n",
    "# for i in new_cols: \n",
    "#     test_data[i] = 0\n",
    "# for i in test_data.index:\n",
    "#     for item in new_cols:\n",
    "#         if item in ast.literal_eval(test_data.loc[i, 'host_verifications']):\n",
    "#             test_data.loc[i, item] = 1\n",
    "test_data = test_data.drop(['host_verifications'], axis=1)\n",
    "\n",
    "# convert amenities to onehot\n",
    "# new_cols = [\n",
    "#     'Smoke alarm','Wifi','Kitchen','Carbon monoxide alarm','Essentials','Hangers','Hair dryer','Hot water',\n",
    "#     'Iron','Dishes and silverware','Shampoo','Cooking basics','Refrigerator','Fire extinguisher','Free parking on premises',\n",
    "#     'Microwave','Bed linens','Dedicated workspace','Heating','Air conditioning','First aid kit','Self check-in','Washer',\n",
    "#     'TV','Dishwasher','Private entrance','Extra pillows and blankets','Long term stays allowed','Free street parking',\n",
    "#     'Coffee maker'\n",
    "# ]\n",
    "# new_cols = [\n",
    "#     'Wifi','Kitchen','Hot water','Free parking on premises','Air conditioning'\n",
    "# ]\n",
    "# for i in new_cols: \n",
    "#     train_data[i] = 0\n",
    "# for i in train_data.index:\n",
    "#     for item in new_cols:\n",
    "#         if item in ast.literal_eval(train_data.loc[i, 'amenities']):\n",
    "#             train_data.loc[i, item] = 1\n",
    "train_data = train_data.drop(['amenities'], axis=1)\n",
    "\n",
    "# for i in new_cols: \n",
    "#     test_data[i] = 0\n",
    "# for i in test_data.index:\n",
    "#     for item in new_cols:\n",
    "#         if item in ast.literal_eval(test_data.loc[i, 'amenities']):\n",
    "#             test_data.loc[i, item] = 1\n",
    "test_data = test_data.drop(['amenities'], axis=1)\n",
    "\n",
    "# only keep year info of host_since\n",
    "train_data['host_since'] = pd.to_datetime(train_data['host_since'])\n",
    "train_data['host_since'] = train_data['host_since'].dt.year + train_data['host_since'].dt.month / 12\n",
    "train_data['host_since'] = train_data['host_since'].astype('float64')\n",
    "\n",
    "test_data['host_since'] = pd.to_datetime(test_data['host_since'])\n",
    "test_data['host_since'] = test_data['host_since'].dt.year + test_data['host_since'].dt.month / 12\n",
    "test_data['host_since'] = test_data['host_since'].astype('float64')\n",
    "\n",
    "# extract numbers from bathrooms_text\n",
    "train_data['bathrooms_text'] = train_data['bathrooms_text'].str.extract('(\\d+\\.?\\d*)').astype('float64')\n",
    "test_data['bathrooms_text'] = test_data['bathrooms_text'].str.extract('(\\d+\\.?\\d*)').astype('float64')\n",
    "\n",
    "# fill nan\n",
    "train_data['bathrooms_text'] = train_data['bathrooms_text'].fillna(train_data['bathrooms_text'].mean())\n",
    "test_data['bathrooms_text'] = test_data['bathrooms_text'].fillna(test_data['bathrooms_text'].mean())\n",
    "\n",
    "# convert these features into binary variables (0: f; 1: t)\n",
    "tran_columns = ['host_is_superhost','host_has_profile_pic','host_identity_verified','has_availability','instant_bookable']\n",
    "for i in range(len(tran_columns)):\n",
    "    train_data.loc[train_data[tran_columns[i]] == 't', tran_columns[i]] = 1\n",
    "    train_data.loc[train_data[tran_columns[i]] == 'f', tran_columns[i]] = 0\n",
    "    train_data[tran_columns[i]] = train_data[tran_columns[i]].astype('float64')\n",
    "\n",
    "    test_data.loc[test_data[tran_columns[i]] == 't', tran_columns[i]] = 1\n",
    "    test_data.loc[test_data[tran_columns[i]] == 'f', tran_columns[i]] = 0\n",
    "    test_data[tran_columns[i]] = test_data[tran_columns[i]].astype('float64')\n",
    "\n",
    "# convert all features into float64\n",
    "for i in train_data.select_dtypes(include=['int64']).columns:\n",
    "    train_data[i] = train_data[i].astype('float64')\n",
    "\n",
    "for i in test_data.select_dtypes(include=['int64']).columns:\n",
    "    test_data[i] = test_data[i].astype('float64')\n",
    "\n",
    "# advance drop cols\n",
    "train_data = train_data.drop(['minimum_nights','maximum_nights','minimum_minimum_nights','maximum_minimum_nights',\n",
    "                              'minimum_maximum_nights','maximum_maximum_nights','availability_60','availability_90',\n",
    "                              'host_has_profile_pic','has_availability'], axis=1)\n",
    "\n",
    "test_data = test_data.drop(['minimum_nights','maximum_nights','minimum_minimum_nights','maximum_minimum_nights',\n",
    "                            'minimum_maximum_nights','maximum_maximum_nights','availability_60','availability_90',\n",
    "                            'host_has_profile_pic','has_availability'], axis=1)\n",
    "\n",
    "# deal with reviews\n",
    "for i in train_data.index:\n",
    "    train_data.loc[i, 'number_of_reviews'] += train_data.loc[i, 'number_of_reviews_ltm'] + train_data.loc[i, 'number_of_reviews_l30d']\n",
    "\n",
    "for i in test_data.index:\n",
    "    test_data.loc[i, 'number_of_reviews'] += test_data.loc[i, 'number_of_reviews_ltm'] + test_data.loc[i, 'number_of_reviews_l30d']\n",
    "\n",
    "train_data = train_data.drop(\"number_of_reviews_ltm\", axis=1)\n",
    "train_data = train_data.drop(\"number_of_reviews_l30d\", axis=1)\n",
    "\n",
    "test_data = test_data.drop(\"number_of_reviews_ltm\", axis=1)\n",
    "test_data = test_data.drop(\"number_of_reviews_l30d\", axis=1)\n",
    "\n",
    "# delete incomplete data\n",
    "train_data = train_data.dropna()\n",
    "# print(train_data.isnull().sum())\n",
    "# print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb5648-2da0-4869-ae98-6228fabea792",
   "metadata": {},
   "source": [
    "## Deal with Data Imbalance in Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d299f1e-16b2-46ef-bd86-0818d1d124bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # draw hist\n",
    "# for i in train_data.columns:\n",
    "#     print(i)\n",
    "#     train_data[i].hist()\n",
    "#     plt.show()\n",
    "\n",
    "# room_type\n",
    "# for i in ['host_is_superhost','host_is_superhost','host_identity_verified','instant_bookable']:\n",
    "#     print(train_data[i].value_counts(sort=False))\n",
    "#     majority, minority = 0, 0\n",
    "#     if train_data[train_data[i] == 0][i].value_counts(sort=False)[0] > train_data[train_data[i] == 1][i].value_counts(sort=False)[1]:\n",
    "#         majority = train_data[train_data[i] == 0]\n",
    "#         minority = train_data[train_data[i] == 1]\n",
    "#     else:\n",
    "#         majority = train_data[train_data[i] == 1]\n",
    "#         minority = train_data[train_data[i] == 0]\n",
    "#     minority_upsampled = resample(minority, replace=True, n_samples=len(majority), random_state=seed)\n",
    "#     train_data = pd.concat([majority, minority_upsampled])\n",
    "#     print(train_data[i].value_counts(sort=False))\n",
    "\n",
    "# # draw hist\n",
    "# for i in train_data.columns:\n",
    "#     print(i)\n",
    "#     train_data[i].hist()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a964d5c9-3f14-4c90-a304-202fdd89db44",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c0ab7d-ab48-4aa0-9627-c0549f0139bf",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb20fa42-6313-4c2a-8c26-b6e8ac78fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "L2 = 1e-3\n",
    "num_classes = 6\n",
    "batch_size = 64\n",
    "EPOCHS = 500\n",
    "CHECKPOINT_FOLDER = \"./saved_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b85791-7734-44d0-8dbf-c8b4d6c7549a",
   "metadata": {},
   "source": [
    "### Get train and val loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d361add-b4cd-406f-8fa9-988bdaa9e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get x and y\n",
    "x = train_data.drop(\"price\", axis=1)\n",
    "y = train_data['price'].astype('int64')\n",
    "\n",
    "# get train and val data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1, train_size=0.9, shuffle=True, stratify=y)\n",
    "\n",
    "x_train['price'] = y_train\n",
    "\n",
    "for i in ['host_is_superhost','host_is_superhost','host_identity_verified','instant_bookable']:\n",
    "    # print(x_train[i].value_counts(sort=False))\n",
    "    majority, minority = 0, 0\n",
    "    if x_train[x_train[i] == 0][i].value_counts(sort=False)[0] > x_train[x_train[i] == 1][i].value_counts(sort=False)[1]:\n",
    "        majority = x_train[x_train[i] == 0]\n",
    "        minority = x_train[x_train[i] == 1]\n",
    "    else:\n",
    "        majority = x_train[x_train[i] == 1]\n",
    "        minority = x_train[x_train[i] == 0]\n",
    "    minority_upsampled = resample(minority, replace=True, n_samples=len(majority), random_state=seed)\n",
    "    x_train = pd.concat([majority, minority_upsampled])\n",
    "    # print(x_train[i].value_counts(sort=False))\n",
    "\n",
    "y_train = x_train['price'].astype('int64')\n",
    "x_train = x_train.drop(\"price\", axis=1)\n",
    "\n",
    "# send data to gpu\n",
    "x_train = torch.tensor(x_train.to_numpy()).float().cuda()\n",
    "y_train = torch.tensor(y_train.to_numpy()).cuda()\n",
    "x_val = torch.tensor(x_val.to_numpy()).float().cuda()\n",
    "y_val = torch.tensor(y_val.to_numpy()).cuda()\n",
    "\n",
    "# normalization\n",
    "x_train_max, _ = torch.max(x_train, axis=0)\n",
    "x_train_min, _ = torch.min(x_train, axis=0)\n",
    "x_train = (x_train - x_train_min) / (x_train_max - x_train_min)\n",
    "x_val = (x_val - x_train_min) / (x_train_max - x_train_min)\n",
    "\n",
    "# create datasets\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "# create data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "# print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8d523c-418a-481b-9702-c87cad092555",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b21868-97c4-4ee8-a709-cde440e5e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClfModel(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(MultiClfModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(num_features, 256)\n",
    "        self.layer2 = nn.Linear(256, 64)\n",
    "        self.layer3 = nn.Linear(64, num_classes)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.layer1(x))\n",
    "        x = self.activation(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dfa891-092b-41ac-a31f-b8292f3e4c34",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f5383a-5e67-4bfb-8552-309b6646146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.torch.set_default_dtype(torch.float32)\n",
    "model = MultiClfModel(num_features=train_data.shape[1] - 1, num_classes=num_classes).cuda()\n",
    "criterion = nn.CrossEntropyLoss()  # because CrossEntropyLoss is usedï¼Œthe output layer don't need softmax\n",
    "optimizer = Adam(model.parameters(), lr=LR, weight_decay=L2)\n",
    "\n",
    "best_val_acc = 0\n",
    "best_val_acc_index = 0\n",
    "for i in range(EPOCHS):\n",
    "    # train\n",
    "    model.train()\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    train_loss = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        correct_examples += (torch.max(output, 1)[1] == targets).sum().item()\n",
    "        total_examples += inputs.shape[0]\n",
    "        train_loss += loss.cpu().detach().numpy()      \n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Epoch %d: Training loss: %.4f, Training accuracy: %.4f\" %(i, avg_loss, avg_acc))\n",
    "\n",
    "    # val\n",
    "    model.eval()\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, targets)\n",
    "            correct_examples += (torch.max(output, 1)[1] == targets).sum().item()\n",
    "            total_examples += inputs.shape[0]\n",
    "            val_loss += loss.cpu().detach().numpy()\n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Epoch %d: Validation loss: %.4f, Validation accuracy: %.4f\" %(i, avg_loss, avg_acc))\n",
    "\n",
    "    # save the model checkpoint\n",
    "    if avg_acc > best_val_acc:\n",
    "        best_val_acc = avg_acc\n",
    "        best_val_acc_index = i\n",
    "        if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "            os.makedirs(CHECKPOINT_FOLDER)\n",
    "        print(\"Saving ...\")\n",
    "        state = {'state_dict': model.state_dict()}\n",
    "        torch.save(state, os.path.join(CHECKPOINT_FOLDER, 'best_model.bin'))\n",
    "    print('')\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Best validation accuracy index: {best_val_acc_index:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e05ec20-5989-4295-bf35-b5740f993b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# y_pred = best_rf_model.predict(test_data)\n",
    "# test_sub = pd.read_csv(\"data/test.csv\")\n",
    "# sub_drop_list = test_sub.columns.tolist()\n",
    "# sub_drop_list.remove('id')\n",
    "# sub_drop_list = pd.Index(sub_drop_list)\n",
    "# test_sub = test_sub.drop(sub_drop_list, axis=1)\n",
    "# test_sub['price'] = pd.DataFrame(y_pred)\n",
    "# test_sub.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
