{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5d51b54-b66c-4b27-8f52-6ded3de01b29",
   "metadata": {},
   "source": [
    "# Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d868e8ca-f1af-4f15-9580-c222aceb087f",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a350f3-b564-4482-83fe-c1f498bcf565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from category_encoders import BinaryEncoder\n",
    "import ast\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.utils import resample\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1851488-ac80-4afb-999f-5e96a8f7e17e",
   "metadata": {},
   "source": [
    "## Set Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d75c673-e59a-4551-a79c-5ddc5cfece6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 666\n",
    "def set_all_seeds(RANDOM_SEED):\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_all_seeds(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56404f-f883-4f2b-bb31-02915475343e",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e88ecc4-2712-4ce4-8f84-cf7e7a977055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# comcat train and test data for data process\n",
    "test_data['price'] = -1\n",
    "train_data = pd.concat([train_data, test_data], axis=0)\n",
    "\n",
    "# initial removal of unwanted features\n",
    "drop_columns = ['id','scrape_id','last_scraped','picture_url','host_id','host_name']\n",
    "train_data = train_data.drop(drop_columns, axis=1)\n",
    "\n",
    "# deal with incomplete data\n",
    "categorical_columns_with_nans = ['host_is_superhost', 'bathrooms_text']\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "train_data[categorical_columns_with_nans] = imputer.fit_transform(train_data[categorical_columns_with_nans])\n",
    "\n",
    "beds_imputer = SimpleImputer(strategy='median')\n",
    "train_data['beds'] = beds_imputer.fit_transform(train_data[['beds']])\n",
    "\n",
    "train_data['description'] = train_data['description'].fillna('')\n",
    "train_data['name'] = train_data['name'].fillna('')\n",
    "\n",
    "# add feature description_length\n",
    "train_data['description_length'] = train_data['description'].apply(len)\n",
    "train_data = train_data.drop('description', axis=1)\n",
    "train_data['name_length'] = train_data['name'].apply(len)\n",
    "train_data = train_data.drop('name', axis=1)\n",
    "\n",
    "# label encode\n",
    "label_encoder = LabelEncoder()\n",
    "train_data['host_is_superhost'] = label_encoder.fit_transform(train_data['host_is_superhost'])\n",
    "\n",
    "# onehot encode\n",
    "categorical_columns = train_data.select_dtypes(include=['object']).columns\n",
    "categorical_columns_to_encode = [col for col in categorical_columns if len(train_data[col].unique()) <= 20]\n",
    "train_data = pd.get_dummies(train_data, columns=categorical_columns_to_encode, drop_first=True)\n",
    "\n",
    "# add feature amenities_count\n",
    "train_data['amenities_count'] = train_data['amenities'].apply(lambda x: len(x.split(',')))\n",
    "train_data = train_data.drop('amenities', axis=1)\n",
    "\n",
    "train_data = pd.get_dummies(train_data, columns=['property_type','neighbourhood_cleansed'], drop_first=True)\n",
    "\n",
    "# only keep year info of host_since\n",
    "train_data['host_since'] = pd.to_datetime(train_data['host_since'])\n",
    "train_data['host_since'] = train_data['host_since'].dt.year + train_data['host_since'].dt.month / 12\n",
    "train_data['host_since'] = train_data['host_since'].astype('float64')\n",
    "\n",
    "# extract numbers from bathrooms_text\n",
    "train_data['bathrooms_shared'] = train_data['bathrooms_text'].isin(['shared', 'Shared'])\n",
    "train_data['bathrooms_private'] = train_data['bathrooms_text'].isin(['private', 'Private'])\n",
    "train_data['bathrooms_text'] = train_data['bathrooms_text'].replace(to_replace='half', value='0.5', regex=True)\n",
    "train_data['bathrooms_text'] = train_data['bathrooms_text'].replace(to_replace='Half', value='0.5', regex=True)\n",
    "train_data['bathrooms_text'] = train_data['bathrooms_text'].str.extract('(\\d+\\.?\\d*)').astype('float64')\n",
    "train_data['bathrooms_text'] = train_data['bathrooms_text'].fillna(train_data['bathrooms_text'].mean())\n",
    "\n",
    "train_data[\"host_verifications_('email', 'phone')\"]=train_data[\"host_verifications_['email', 'phone']\"]\n",
    "train_data[\"host_verifications_('email', 'work_email')\"]=train_data[\"host_verifications_['email', 'work_email']\"]\n",
    "train_data[\"host_verifications_('email')\"]=train_data[\"host_verifications_['email']\"]\n",
    "train_data[\"host_verifications_('phone', 'work_email')\"]=train_data[\"host_verifications_['phone', 'work_email']\"]\n",
    "train_data[\"host_verifications_('phone')\"]=train_data[\"host_verifications_['phone']\"]\n",
    "train_data[\"host_verifications_()\"]=train_data[\"host_verifications_[]\"]\n",
    "train_data = train_data.drop(\"host_verifications_['email', 'phone']\", axis=1)\n",
    "train_data = train_data.drop(\"host_verifications_['email', 'work_email']\", axis=1)\n",
    "train_data = train_data.drop(\"host_verifications_['email']\", axis=1)\n",
    "train_data = train_data.drop(\"host_verifications_['phone', 'work_email']\", axis=1)\n",
    "train_data = train_data.drop(\"host_verifications_['phone']\", axis=1)\n",
    "train_data = train_data.drop(\"host_verifications_[]\", axis=1)\n",
    "\n",
    "# train_data['number_of_reviews'] += train_data['number_of_reviews_ltm'] + train_data['number_of_reviews_l30d']\n",
    "# train_data = train_data.drop(\"number_of_reviews_ltm\", axis=1)\n",
    "# train_data = train_data.drop(\"number_of_reviews_l30d\", axis=1)\n",
    "\n",
    "# split test and train data\n",
    "test_data = train_data[train_data['price'] == -1].drop(columns=['price'])\n",
    "train_data = train_data[train_data['price'] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c75734-2d55-4456-bba9-3899691a1e11",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d466850-6850-4e9b-9c09-2ee22f870d38",
   "metadata": {},
   "source": [
    "### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae863424-1bbd-42d0-a1fb-54704fa44dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train data for RF\n",
    "x_train = train_data.drop(\"price\", axis=1)\n",
    "y_train = train_data['price'].astype('int64')\n",
    "\n",
    "# params to tune\n",
    "# xgboost_hyperparam = {\n",
    "#     'booster': ['gbtree'],\n",
    "#     'tree_method': ['auto'],\n",
    "#     'n_estimators': [500],\n",
    "#     'eta': [0.05],\n",
    "#     'max_depth': [10],\n",
    "#     'gamma': [0.5]\n",
    "# }\n",
    "\n",
    "xgboost_hyperparam = {\n",
    "    'booster': ['gbtree'],\n",
    "    'tree_method': ['auto'],\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [10],\n",
    "    'learning_rate' : [0.05],\n",
    "    'reg_lambda': [1],\n",
    "    'gamma': [0.5]\n",
    "}\n",
    "\n",
    "xgboost_model = XGBClassifier(random_state=seed)\n",
    "xgboost_grid = GridSearchCV(xgboost_model, xgboost_hyperparam, cv=5,scoring='f1_macro')\n",
    "xgboost_grid.fit(x_train, y_train)\n",
    "print(xgboost_grid.best_score_)\n",
    "print(xgboost_grid.best_params_)\n",
    "best_xgboost_model = xgboost_grid.best_estimator_\n",
    "\n",
    "scores = cross_val_score(best_xgboost_model, x_train, y_train, cv=5, scoring='accuracy')\n",
    "print(scores.mean())\n",
    "# 0.5662197635936979"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86f4baf-ee58-4990-ad26-02474551c457",
   "metadata": {},
   "source": [
    "### Deal with Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837ba087-de2e-4043-bc10-ee73dda4ce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get train data for RF\n",
    "# x_train = train_data.drop(\"price\", axis=1)\n",
    "# y_train = train_data['price'].astype('int64')\n",
    "\n",
    "# # deal with imbalanced data\n",
    "# smote = SMOTE(random_state=seed)\n",
    "# x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "# # print(Counter(y_train))\n",
    "\n",
    "# best_xgboost_model = XGBClassifier(n_estimators=500,max_depth=10,learning_rate=0.05,reg_lambda=1,random_state=seed)\n",
    "# best_xgboost_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91877ab3-bf86-4f84-a7d2-d61345b3512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# y_pred = best_xgboost_model.predict(test_data)\n",
    "# test_sub = pd.read_csv(\"data/test.csv\")\n",
    "# sub_drop_list = test_sub.columns.tolist()\n",
    "# sub_drop_list.remove('id')\n",
    "# sub_drop_list = pd.Index(sub_drop_list)\n",
    "# test_sub = test_sub.drop(sub_drop_list, axis=1)\n",
    "# test_sub['price'] = pd.DataFrame(y_pred)\n",
    "# test_sub.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a5fdd-8ff9-4c2b-81b5-aab5bd9ead96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
