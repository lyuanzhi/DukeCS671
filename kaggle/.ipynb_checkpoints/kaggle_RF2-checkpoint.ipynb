{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5d51b54-b66c-4b27-8f52-6ded3de01b29",
   "metadata": {},
   "source": [
    "# Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d868e8ca-f1af-4f15-9580-c222aceb087f",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a350f3-b564-4482-83fe-c1f498bcf565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from category_encoders import BinaryEncoder\n",
    "import ast\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.utils import resample\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1851488-ac80-4afb-999f-5e96a8f7e17e",
   "metadata": {},
   "source": [
    "## Set Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d75c673-e59a-4551-a79c-5ddc5cfece6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 666\n",
    "def set_all_seeds(RANDOM_SEED):\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_all_seeds(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56404f-f883-4f2b-bb31-02915475343e",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e88ecc4-2712-4ce4-8f84-cf7e7a977055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# comcat train and test data for data process\n",
    "test_data['price'] = -1\n",
    "train_data = pd.concat([train_data, test_data], axis=0)\n",
    "\n",
    "# initial removal of unwanted features\n",
    "drop_columns = ['id','scrape_id','last_scraped','picture_url','host_id','host_name','name']\n",
    "train_data = train_data.drop(drop_columns, axis=1)\n",
    "\n",
    "# deal with incomplete data\n",
    "categorical_columns_with_nans = ['host_is_superhost', 'bathrooms_text']\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "train_data[categorical_columns_with_nans] = imputer.fit_transform(train_data[categorical_columns_with_nans])\n",
    "\n",
    "beds_imputer = SimpleImputer(strategy='median')\n",
    "train_data['beds'] = beds_imputer.fit_transform(train_data[['beds']])\n",
    "\n",
    "train_data['description'] = train_data['description'].fillna('')\n",
    "\n",
    "# add feature description_length\n",
    "train_data['description_length'] = train_data['description'].apply(len)\n",
    "train_data = train_data.drop('description', axis=1)\n",
    "\n",
    "# label encode\n",
    "label_encoder = LabelEncoder()\n",
    "train_data['host_is_superhost'] = label_encoder.fit_transform(train_data['host_is_superhost'])\n",
    "\n",
    "# onehot encode\n",
    "categorical_columns = train_data.select_dtypes(include=['object']).columns\n",
    "categorical_columns_to_encode = [col for col in categorical_columns if len(train_data[col].unique()) <= 20]\n",
    "train_data = pd.get_dummies(train_data, columns=categorical_columns_to_encode, drop_first=True)\n",
    "\n",
    "# add feature amenities_count\n",
    "train_data['amenities_count'] = train_data['amenities'].apply(lambda x: len(x.split(',')))\n",
    "train_data = train_data.drop('amenities', axis=1)\n",
    "\n",
    "train_data = pd.get_dummies(train_data, columns=['property_type','neighbourhood_cleansed'], drop_first=True)\n",
    "\n",
    "# only keep year info of host_since\n",
    "train_data['host_since'] = pd.to_datetime(train_data['host_since'])\n",
    "train_data['host_since'] = train_data['host_since'].dt.year + train_data['host_since'].dt.month / 12\n",
    "train_data['host_since'] = train_data['host_since'].astype('float64')\n",
    "\n",
    "# # extract numbers from bathrooms_text\n",
    "train_data['bathrooms_text'] = train_data['bathrooms_text'].str.extract('(\\d+\\.?\\d*)').astype('float64')\n",
    "train_data['bathrooms_text'] = train_data['bathrooms_text'].fillna(train_data['bathrooms_text'].mean())\n",
    "\n",
    "# split test and train data\n",
    "test_data = train_data[train_data['price'] == -1].drop(columns=['price'])\n",
    "train_data = train_data[train_data['price'] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c75734-2d55-4456-bba9-3899691a1e11",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86f4baf-ee58-4990-ad26-02474551c457",
   "metadata": {},
   "source": [
    "### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "837ba087-de2e-4043-bc10-ee73dda4ce09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None, 'n_estimators': 500}\n",
      "0.5662843417307277\n"
     ]
    }
   ],
   "source": [
    "# get train data for RF\n",
    "x_train = train_data.drop(\"price\", axis=1)\n",
    "y_train = train_data['price'].astype('int64')\n",
    "\n",
    "# params to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [500],\n",
    "    'max_depth': [None]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=seed)\n",
    "grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "grid_search_rf.fit(x_train, y_train)\n",
    "print(grid_search_rf.best_params_)\n",
    "print(grid_search_rf.best_score_)\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "# 0.568032731890026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f55284e0-9b90-4e05-9597-d39225fed21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from tensorflow.keras.regularizers import l2\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# X = train_data.drop('price', axis=1)\n",
    "# y = train_data['price']\n",
    "\n",
    "# # Splitting the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Standardize the features (mean=0, std=1)\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Convert the target variable to categorical (one-hot encoding)\n",
    "# y_train_categorical = to_categorical(y_train)\n",
    "# y_test_categorical = to_categorical(y_test)\n",
    "\n",
    "# # Learning rate schedule function\n",
    "# def lr_schedule(epoch, lr):\n",
    "#     \"\"\" Learning rate is scheduled to be reduced after 40, 80, 120, 160 epochs.\"\"\"\n",
    "#     decay_rate = 0.1\n",
    "#     decay_step = 40\n",
    "#     if epoch % decay_step == 0 and epoch:\n",
    "#         return lr * decay_rate\n",
    "#     return lr\n",
    "\n",
    "# # Define the improved MLP model with a more complex architecture\n",
    "# def create_improved_model(input_dim, output_dim, reg_lambda):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(256, input_dim=input_dim, activation='relu', kernel_regularizer=l2(reg_lambda)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.5))\n",
    "    \n",
    "#     model.add(Dense(128, activation='relu', kernel_regularizer=l2(reg_lambda)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.5))\n",
    "    \n",
    "#     model.add(Dense(64, activation='relu', kernel_regularizer=l2(reg_lambda)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.5))\n",
    "    \n",
    "#     model.add(Dense(32, activation='relu', kernel_regularizer=l2(reg_lambda)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.5))\n",
    "    \n",
    "#     model.add(Dense(output_dim, activation='softmax'))\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Initialize the improved model\n",
    "# improved_model = create_improved_model(X_train_scaled.shape[1], y_train_categorical.shape[1], reg_lambda=0.001)\n",
    "\n",
    "# # Compile the improved model with Adam optimizer\n",
    "# improved_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Define the learning rate scheduler callback\n",
    "# lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# # Define early stopping callback\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# # Train the improved model with early stopping and learning rate scheduler\n",
    "# history = improved_model.fit(X_train_scaled, y_train_categorical, \n",
    "#                              validation_split=0.2, \n",
    "#                              epochs=200,  # Increased number of epochs\n",
    "#                              batch_size=64, \n",
    "#                              callbacks=[early_stopping, lr_scheduler],\n",
    "#                              verbose=0)\n",
    "\n",
    "# # Evaluate the improved model on the test data\n",
    "# loss, accuracy = improved_model.evaluate(X_test_scaled, y_test_categorical, verbose=0)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_pred_categorical = improved_model.predict(X_test_scaled)\n",
    "# y_pred = y_pred_categorical.argmax(axis=1)\n",
    "\n",
    "# # Calculate the confusion matrix\n",
    "# conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # Output the accuracy and confusion matrix\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91877ab3-bf86-4f84-a7d2-d61345b3512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "y_pred = best_rf_model.predict(test_data)\n",
    "test_sub = pd.read_csv(\"data/test.csv\")\n",
    "sub_drop_list = test_sub.columns.tolist()\n",
    "sub_drop_list.remove('id')\n",
    "sub_drop_list = pd.Index(sub_drop_list)\n",
    "test_sub = test_sub.drop(sub_drop_list, axis=1)\n",
    "test_sub['price'] = pd.DataFrame(y_pred)\n",
    "test_sub.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
