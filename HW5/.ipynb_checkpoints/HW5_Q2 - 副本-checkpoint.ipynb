{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vV8Z-_edZoAo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import necessary dependencies\n",
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_all_seeds(RANDOM_SEED):\n",
    "    random.seed(RANDOM_SEED)     # python random generator\n",
    "    np.random.seed(RANDOM_SEED)  # numpy random generator\n",
    "\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_all_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BwOSbMuZZoAp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleCIFAR10Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCIFAR10Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, 5)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3)\n",
    "        self.fc1   = nn.Linear(16*6*6, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50000 5000 5000\n"
     ]
    }
   ],
   "source": [
    "# useful libraries\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "##################\n",
    "# YOUR CODE HERE #\n",
    "##################\n",
    "\n",
    "# adjust batch size to your need\n",
    "batch_size = 64\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "val_size = int(0.5 * len(testset))\n",
    "test_size = len(testset) - val_size\n",
    "valset, testset = torch.utils.data.random_split(testset, [val_size, test_size])\n",
    "\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "print(len(trainset), len(valset), len(testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJ4YB7QvV098",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "iivPVzUvZoAu",
    "outputId": "c8fe0d49-0b93-41aa-b456-ae03ab910a5e",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Training loss: 1.8587, Training accuracy: 0.3090\n",
      "Validation loss: 1.5058, Validation accuracy: 0.4594\n",
      "Saving ...\n",
      "\n",
      "Epoch 1:\n",
      "Training loss: 1.3745, Training accuracy: 0.5023\n",
      "Validation loss: 1.3045, Validation accuracy: 0.5366\n",
      "Saving ...\n",
      "\n",
      "Epoch 2:\n",
      "Training loss: 1.1939, Training accuracy: 0.5760\n",
      "Validation loss: 1.1641, Validation accuracy: 0.5888\n",
      "Saving ...\n",
      "\n",
      "Epoch 3:\n",
      "Training loss: 1.0741, Training accuracy: 0.6174\n",
      "Validation loss: 1.1005, Validation accuracy: 0.6140\n",
      "Saving ...\n",
      "\n",
      "Epoch 4:\n",
      "Training loss: 0.9947, Training accuracy: 0.6489\n"
     ]
    }
   ],
   "source": [
    "net = SimpleCIFAR10Classifier().cuda()\n",
    "INITIAL_LR = 0.01\n",
    "MOMENTUM = 0.9\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=MOMENTUM)\n",
    "\n",
    "EPOCHS = 30\n",
    "CHECKPOINT_FOLDER = \"./saved_model\"\n",
    "best_val_acc = 0\n",
    "\n",
    "# Training Loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for i in range(0, EPOCHS):\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    print(\"Epoch %d:\" %i)\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "\n",
    "    # Record training loss\n",
    "    train_loss = 0 \n",
    "\n",
    "    # Looping through training loader\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        ##################\n",
    "        # YOUR CODE HERE #\n",
    "        ##################\n",
    "\n",
    "        # Send input and target to device\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        # compute the model output logits and training loss\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        # back propogation & optimizer update parametes\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # calculate predictions\n",
    "        _, predictions = outputs.max(1)\n",
    "        correct_examples += (predictions == targets).sum().item()\n",
    "        total_examples += inputs.shape[0]\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # calculate average training loss and accuracy\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    \n",
    "    # Evaluate the validation set performance\n",
    "    net.eval()\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "\n",
    "    # Record validation loss\n",
    "    val_loss = 0\n",
    "\n",
    "    # disable gradient during validation, which can save GPU memory\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            ##################\n",
    "            # YOUR CODE HERE #\n",
    "            ##################\n",
    "            \n",
    "            # Send input and target to device\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            # compute the model output logits and training loss\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # count the number of correctly predicted samples in the current batch\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            correct_examples += (predictions == targets).sum().item()\n",
    "            total_examples += inputs.shape[0]\n",
    "            val_loss += loss.cpu().detach().numpy()\n",
    "\n",
    "    # calculate average validation loss and accuracy\n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "    val_losses.append(avg_loss)\n",
    "    \n",
    "    # save the model checkpoint\n",
    "    current_learning_rate = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    if avg_acc > best_val_acc:\n",
    "        best_val_acc = avg_acc\n",
    "\n",
    "        if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "           os.makedirs(CHECKPOINT_FOLDER)\n",
    "        print(\"Saving ...\")\n",
    "        state = {'state_dict': net.state_dict(),\n",
    "                'epoch': i,\n",
    "                'lr': current_learning_rate}\n",
    "        torch.save(state, os.path.join(CHECKPOINT_FOLDER, 'best_model.bin'))\n",
    "\n",
    "    print('')\n",
    "\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# YOUR CODE HERE #\n",
    "##################\n",
    "\n",
    "# load trained model weight\n",
    "net = SimpleCIFAR10Classifier().cuda()\n",
    "net.load_state_dict(torch.load(os.path.join(CHECKPOINT_FOLDER, 'best_model.bin'))['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# YOUR CODE HERE #\n",
    "##################\n",
    "\n",
    "# write another loop to evaluate trained model performance on the test split\n",
    "net.eval()\n",
    "total_examples = 0\n",
    "correct_examples = 0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        outputs = net(inputs)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        correct_examples += (predictions == targets).sum().item()\n",
    "        total_examples += inputs.shape[0]\n",
    "\n",
    "avg_acc = correct_examples / total_examples\n",
    "print(\"Testing accuracy: %.4f\" % (avg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# YOUR CODE HERE #\n",
    "##################\n",
    "\n",
    "# visualize model loss curves (both train and loss)\n",
    "plt.plot(range(0, EPOCHS), train_losses, color='r')\n",
    "plt.plot(range(0, EPOCHS), val_losses, color='b')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['training loss', 'validation loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above figure, when the epoch increases, the training loss decreases, while the validation loss decreases and then increases, which means that this model is overfitting. By comparing training accuracy and testing accuracy, we can also draw the conclusion of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# (C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "INITIAL_LR = 0.01\n",
    "net = SimpleCIFAR10Classifier().cuda()\n",
    "MOMENTUM = 0.9\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=MOMENTUM)\n",
    "\n",
    "EPOCHS = 30\n",
    "CHECKPOINT_FOLDER = \"./saved_model\"\n",
    "\n",
    "##################\n",
    "# YOUR CODE HERE #\n",
    "##################\n",
    "\n",
    "# set your own L1 regularization weight \n",
    "REG = 1e-4\n",
    "\n",
    "# write training loops with L1 regularization and validation loops (Hint: similar to (a))\n",
    "best_val_acc = 0\n",
    "\n",
    "# Training Loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for i in range(0, EPOCHS):\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    print(\"Epoch %d:\" %i)\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "\n",
    "    # Record training loss\n",
    "    train_loss = 0 \n",
    "\n",
    "    # Looping through training loader\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # Send input and target to device\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        # compute the model output logits and training loss\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # calculate L1 term\n",
    "        L1_term = torch.tensor(0.).cuda()\n",
    "        for name, weights in net.named_parameters():\n",
    "            if 'bias' not in name:\n",
    "                L1_term += torch.norm(weights, 1)\n",
    "\n",
    "        CE_term = criterion(outputs, targets)\n",
    "        loss = CE_term + REG * L1_term\n",
    "\n",
    "        # back propogation & optimizer update parametes\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # calculate predictions\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        correct_examples += (predictions == targets).sum().item()\n",
    "        total_examples += inputs.shape[0]\n",
    "        train_loss += loss.cpu().detach().numpy()\n",
    "\n",
    "    # calculate average training loss and accuracy\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    \n",
    "    # Evaluate the validation set performance\n",
    "    net.eval()\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "\n",
    "    # Record validation loss\n",
    "    val_loss = 0\n",
    "\n",
    "    # disable gradient during validation, which can save GPU memory\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # Send input and target to device\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            # compute the model output logits and training loss\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # count the number of correctly predicted samples in the current batch\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            correct_examples += (predictions == targets).sum().item()\n",
    "            total_examples += inputs.shape[0]\n",
    "            val_loss += loss.cpu().detach().numpy()\n",
    "\n",
    "    # calculate average validation loss and accuracy\n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "    val_losses.append(avg_loss)\n",
    "    \n",
    "    # save the model checkpoint\n",
    "    current_learning_rate = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    if avg_acc > best_val_acc:\n",
    "        best_val_acc = avg_acc\n",
    "\n",
    "        if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "           os.makedirs(CHECKPOINT_FOLDER)\n",
    "        print(\"Saving ...\")\n",
    "        state = {'state_dict': net.state_dict(),\n",
    "                'epoch': i,\n",
    "                'lr': current_learning_rate}\n",
    "        torch.save(state, os.path.join(CHECKPOINT_FOLDER, 'best_model_L1.bin'))\n",
    "\n",
    "    print('')\n",
    "\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# YOUR CODE HERE #\n",
    "##################\n",
    "\n",
    "# load trained model weight\n",
    "net = SimpleCIFAR10Classifier().cuda()\n",
    "net.load_state_dict(torch.load(os.path.join(CHECKPOINT_FOLDER, 'best_model_L1.bin'))['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# YOUR CODE HERE #\n",
    "##################\n",
    "\n",
    "# write another loop to evaluate trained model performance on the test split\n",
    "net.eval()\n",
    "total_examples = 0\n",
    "correct_examples = 0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        outputs = net(inputs)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        correct_examples += (predictions == targets).sum().item()\n",
    "        total_examples += inputs.shape[0]\n",
    "\n",
    "avg_acc = correct_examples / total_examples\n",
    "print(\"Testing accuracy: %.4f\" % (avg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# YOUR CODE HERE #\n",
    "##################\n",
    "\n",
    "# visualize model loss curves (both train and loss)\n",
    "plt.plot(range(0, EPOCHS), train_losses, color='r')\n",
    "plt.plot(range(0, EPOCHS), val_losses, color='b')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['training loss', 'validation loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows that this model is less overfitting than the model without L1 regularization. And the testing accuracy of this model (0.6844) is larger than the model without L1 regularization (0.6512). Therefore, this model is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Visualize the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.subplots_adjust(wspace = 0.4, hspace = 0.4)\n",
    "plt_i = 0\n",
    "for name, module in net.named_modules():\n",
    "    if 'conv' in name or 'fc' in name:\n",
    "        ##################\n",
    "        # YOUR CODE HERE #\n",
    "        ##################\n",
    "      \n",
    "        # extract weight from layers\n",
    "        weight = module.weight.cpu().detach().numpy().flatten()\n",
    "    \n",
    "        # Visualize the weights\n",
    "        plt_i += 1\n",
    "        plt.subplot(230 + plt_i)\n",
    "        _ = plt.hist(weight, bins=20)\n",
    "        plt.title(\"Weight histogram of layer \" + name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "INITIAL_LR = 0.01\n",
    "net = SimpleCIFAR10Classifier().cuda()\n",
    "MOMENTUM = 0.9\n",
    "REG = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "EPOCHS = 30\n",
    "CHECKPOINT_FOLDER = \"./saved_model\"\n",
    "\n",
    "##################\n",
    "# YOUR CODE HERE #\n",
    "##################\n",
    "\n",
    "# set your own L2 regularization weight \n",
    "REG = 1e-3 \n",
    "optimizer = optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay=REG)\n",
    "\n",
    "# write training loops with L2 regularization and validation loops (Hint: similar to (a))\n",
    "best_val_acc = 0\n",
    "\n",
    "# Training Loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for i in range(0, EPOCHS):\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    print(\"Epoch %d:\" %i)\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "\n",
    "    # Record training loss\n",
    "    train_loss = 0 \n",
    "\n",
    "    # Looping through training loader\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # Send input and target to device\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        # compute the model output logits and training loss\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        # back propogation & optimizer update parametes\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # calculate predictions\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        correct_examples += (predictions == targets).sum().item()\n",
    "        total_examples += inputs.shape[0]\n",
    "        train_loss += loss.cpu().detach().numpy()\n",
    "\n",
    "    # calculate average training loss and accuracy\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    \n",
    "    # Evaluate the validation set performance\n",
    "    net.eval()\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "\n",
    "    # Record validation loss\n",
    "    val_loss = 0\n",
    "\n",
    "    # disable gradient during validation, which can save GPU memory\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # Send input and target to device\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            # compute the model output logits and training loss\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # count the number of correctly predicted samples in the current batch\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            correct_examples += (predictions == targets).sum().item()\n",
    "            total_examples += inputs.shape[0]\n",
    "            val_loss += loss.cpu().detach().numpy()\n",
    "\n",
    "    # calculate average validation loss and accuracy\n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "    val_losses.append(avg_loss)\n",
    "    \n",
    "    # save the model checkpoint\n",
    "    current_learning_rate = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    if avg_acc > best_val_acc:\n",
    "        best_val_acc = avg_acc\n",
    "\n",
    "        if not os.path.exists(CHECKPOINT_FOLDER):\n",
    "           os.makedirs(CHECKPOINT_FOLDER)\n",
    "        print(\"Saving ...\")\n",
    "        state = {'state_dict': net.state_dict(),\n",
    "                'epoch': i,\n",
    "                'lr': current_learning_rate}\n",
    "        torch.save(state, os.path.join(CHECKPOINT_FOLDER, 'best_model_L2.bin'))\n",
    "\n",
    "    print('')\n",
    "\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# YOUR CODE HERE #\n",
    "##################\n",
    "\n",
    "# load trained model weight\n",
    "net = SimpleCIFAR10Classifier().cuda()\n",
    "net.load_state_dict(torch.load(os.path.join(CHECKPOINT_FOLDER, 'best_model_L2.bin'))['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# YOUR CODE HERE #\n",
    "##################\n",
    "\n",
    "# write another loop to evaluate trained model performance on the test split\n",
    "net.eval()\n",
    "total_examples = 0\n",
    "correct_examples = 0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        outputs = net(inputs)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        correct_examples += (predictions == targets).sum().item()\n",
    "        total_examples += inputs.shape[0]\n",
    "\n",
    "avg_acc = correct_examples / total_examples\n",
    "print(\"Testing accuracy: %.4f\" % (avg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# YOUR CODE HERE #\n",
    "##################\n",
    "\n",
    "# visualize model loss curves (both train and loss)\n",
    "plt.plot(range(0, EPOCHS), train_losses, color='r')\n",
    "plt.plot(range(0, EPOCHS), val_losses, color='b')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['training loss', 'validation loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Visualize the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.subplots_adjust(wspace = 0.4, hspace = 0.4)\n",
    "plt_i = 0\n",
    "for name, module in net.named_modules():\n",
    "    if 'conv' in name or 'fc' in name:\n",
    "        ##################\n",
    "        # YOUR CODE HERE #\n",
    "        ##################\n",
    "      \n",
    "        # extract weight from layers\n",
    "        weight = module.weight.cpu().detach().numpy().flatten()\n",
    "    \n",
    "        # Visualize the weights\n",
    "        plt_i += 1\n",
    "        plt.subplot(230 + plt_i)\n",
    "        _ = plt.hist(weight, bins=20)\n",
    "        plt.title(\"Weight histogram of layer \"+name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### comment on the differences between L1 and L2 regularization. \n",
    "#### The weight histograms for L1 regularization exhibit sharp spikes at zero, while the weights for L2 regularization are more evenly spread around zero. \n",
    "#### Effects on model weights: L1 regularization penalizes the absolute value of the weights, which compels the model to be sparse; while L2 penalizes the square of the weights, which encourages tiny weights but doesn't impose sparsity.\n",
    "#### Effects on model performance: L1 Regularization sets many weights to zero, which lead to a simpler model. It works well if the dataset has irrelevant or redundant features. In comparison, L2 Regularization keeps more features, potentially improving model performance; nevertheless, failure to properly adjust the REG increases the danger of overfitting."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "b11t8AObPAG_",
    "Zk2ykaHlSgZ5",
    "1Pz5T6QRWQ92"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
