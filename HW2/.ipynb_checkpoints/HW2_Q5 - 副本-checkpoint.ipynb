{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51aa32f5-dc1f-4ed2-ab49-aca39b79b7c6",
   "metadata": {},
   "source": [
    "# Q5 Decision Trees Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51240f4-748a-447e-9290-9a12f1546e73",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "679fa44b-d70a-4701-bf6d-8c1d6347813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import cross_validate, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gosdt.model.gosdt import GOSDT\n",
    "from gosdt.model.threshold_guess import compute_thresholds, cut\n",
    "\n",
    "# set random seed as 42\n",
    "np.random.seed(42)\n",
    "\n",
    "# read data from csv file\n",
    "data = pd.read_csv(\"kindey stone urine analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97646812-c924-4c91-bcc8-9f935c1e3a1e",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6040f421-2634-43ae-becc-288cb74b0649",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shuffle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# shuffle\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m shuffle_data \u001b[38;5;241m=\u001b[39m \u001b[43mshuffle\u001b[49m(data, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# print(data)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# print(shuffle_data)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m x \u001b[38;5;241m=\u001b[39m shuffle_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgravity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mph\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mosmo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcond\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124murea\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcalc\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'shuffle' is not defined"
     ]
    }
   ],
   "source": [
    "# shuffle\n",
    "shuffle_data = shuffle(data, random_state=42)\n",
    "# print(data)\n",
    "# print(shuffle_data)\n",
    "x = shuffle_data[['gravity', 'ph', 'osmo', 'cond', 'urea', 'calc']]\n",
    "y = shuffle_data.target\n",
    "# print(x)\n",
    "# print(y)\n",
    "\n",
    "accuracy_scores, f1_scores, auc_scores, max_depths = [], [], [], []\n",
    "best_accuracy, best_f1, best_auc = 0, 0, 0\n",
    "best_accuracy_depth, best_f1_depth, best_auc_depth = 0, 0, 0\n",
    "\n",
    "# tuning max depth from 1 to 10\n",
    "for i in range(10):\n",
    "    model = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=i+1, random_state=42)\n",
    "    scores = cross_validate(model, x, y, scoring=['accuracy', 'f1', 'roc_auc'], cv=5)\n",
    "\n",
    "    print(scores['test_accuracy'])\n",
    "\n",
    "    accuracy_scores.append(np.mean(scores['test_accuracy']))\n",
    "    f1_scores.append(np.mean(scores['test_f1']))\n",
    "    auc_scores.append(np.mean(scores['test_roc_auc']))\n",
    "    max_depths.append(i+1)\n",
    "\n",
    "    # choose best max depth\n",
    "    if best_accuracy < np.mean(scores['test_accuracy']):\n",
    "        best_accuracy = np.mean(scores['test_accuracy'])\n",
    "        best_accuracy_depth = i + 1\n",
    "    if best_f1 < np.mean(scores['test_f1']):\n",
    "        best_f1 = np.mean(scores['test_f1'])\n",
    "        best_f1_depth = i + 1\n",
    "    if best_auc < np.mean(scores['test_roc_auc']):\n",
    "        best_auc = np.mean(scores['test_roc_auc'])\n",
    "        best_auc_depth = i + 1\n",
    "\n",
    "# plot relationship between max depth and average model performance\n",
    "plt.plot(max_depths, accuracy_scores, color='r')\n",
    "plt.plot(max_depths, f1_scores, color='g')\n",
    "plt.plot(max_depths, auc_scores, color='b')\n",
    "plt.xlabel('Maximum Depth')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(['Accuracy', 'F1', 'AUC'])\n",
    "plt.show()\n",
    "\n",
    "# best max depth for different model performance\n",
    "print('When accuracy is the highest, the max_depth is', best_accuracy_depth)\n",
    "print('When F1 score is the highest, the max_depth is', best_f1_depth)\n",
    "print('When AUC is the highest, the max_depth is', best_auc_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a523548-9db6-4ac7-8a81-058bc0487a99",
   "metadata": {},
   "source": [
    "# (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d51dc87-4356-4f49-b93c-dcc9f72d353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test dataset\n",
    "x = data[['gravity', 'ph', 'osmo', 'cond', 'urea', 'calc']]\n",
    "y = data.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, train_size=0.8, random_state=42, shuffle=True, stratify=y)\n",
    "\n",
    "# GridSearch 5-fold Cross Validation on criterion, splitter, and max_depth\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "param = {'criterion':['gini', 'entropy', 'log_loss'], 'splitter':['best', 'random'], 'max_depth':[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "\n",
    "# custom refit strategy\n",
    "def custom_refit_strategy(cv_results):\n",
    "    cv_results_ = pd.DataFrame(cv_results)\n",
    "    return np.argmax(cv_results_[\"mean_test_accuracy\"] + cv_results_[\"mean_test_f1\"] + cv_results_[\"mean_test_roc_auc\"])\n",
    "\n",
    "model = GridSearchCV(model, param_grid=param, scoring=['accuracy', 'f1', 'roc_auc'], cv=5, refit=custom_refit_strategy)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# find the best parameters\n",
    "print('The best parameters:', model.best_params_)\n",
    "# print(model.best_index_)\n",
    "\n",
    "# train the whole training set\n",
    "model = model.best_estimator_\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# plot the tree\n",
    "plt.figure(figsize=(12,12))\n",
    "_ = plot_tree(\n",
    "    model, \n",
    "    feature_names=['gravity', 'ph', 'osmo', 'cond', 'urea', 'calc'],  \n",
    "    class_names=['0', '1'], # target\n",
    "    filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b01ec-2682-4be9-81d7-6fffd5c17d70",
   "metadata": {},
   "source": [
    "# (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b2958-8353-413d-bda7-032c0d22a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os, sys\n",
    "\n",
    "# ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# ignore gosdt's print information\n",
    "class ignorePrint:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "# generate binary features\n",
    "x_train_bin, thresholds, header, _ = compute_thresholds(x_train.copy(), y_train, 40, 1)\n",
    "# print(x_train_bin.shape)\n",
    "x_test_bin = cut(x_test.copy(), thresholds)\n",
    "x_test_bin = x_test_bin[header]\n",
    "# print(x_test_bin.shape, x_train_bin.shape)\n",
    "\n",
    "# train\n",
    "model = GOSDT({\n",
    "    'regularization': 0.02,\n",
    "    'depth_budget': 5,\n",
    "    'similar_support': False\n",
    "})\n",
    "with ignorePrint():\n",
    "    model.fit(x_train_bin, y_train)\n",
    "\n",
    "# test GOSDT\n",
    "y_pred = model.predict(x_test_bin)\n",
    "# F1 scores and AUC Scores on test dataset\n",
    "print('\\nF1 score on test dataset (GOSDT):', f1_score(y_test, y_pred))\n",
    "print('AUC score on test dataset (GOSDT):', roc_auc_score(y_test, y_pred), '\\n')\n",
    "\n",
    "# tuning depth_budget with 5-fold cross-validation\n",
    "# create 5 folds evenly\n",
    "folds_index = []\n",
    "x_train_bin_split = x_train_bin.copy()\n",
    "n_int = x_train_bin_split.shape[0] // 5\n",
    "n_remain = x_train_bin_split.shape[0] % 5\n",
    "# print(n_int, n_remain)\n",
    "for i in range(5):\n",
    "    n = n_int\n",
    "    if n_remain != 0:\n",
    "        n += 1\n",
    "        n_remain -= 1\n",
    "    index = x_train_bin_split.head(n)._stat_axis.values.tolist()\n",
    "    folds_index.append(index)\n",
    "    x_train_bin_split.drop(index, axis=0, inplace=True)\n",
    "\n",
    "# tuning depth_budget (k + 1)\n",
    "F1_avg_train_scores, accuracy_test_scores, F1_test_scores, depth_budgets = [], [], [], []\n",
    "# k_lis = [0.001, 0.01, 0.1, 0.3, 0.5]\n",
    "for k in range(10):\n",
    "    model = GOSDT({\n",
    "        'regularization': 0.02,\n",
    "        'depth_budget': k+1,\n",
    "        'similar_support': False\n",
    "    })\n",
    "    F1_sum_cv = 0\n",
    "    for i in range(5):\n",
    "        eval_index_cv = folds_index[i]\n",
    "        train_index_cv = []\n",
    "        for j in range(5):\n",
    "            if i != j:\n",
    "                train_index_cv += folds_index[j]\n",
    "        \n",
    "        #get eval and train set\n",
    "        eval_x_cv = x_train_bin.loc[eval_index_cv]\n",
    "        eval_y_cv = y_train.loc[eval_index_cv]\n",
    "        train_x_cv = x_train_bin.loc[train_index_cv]\n",
    "        train_y_cv = y_train.loc[train_index_cv]\n",
    "        \n",
    "        with ignorePrint():\n",
    "            model.fit(train_x_cv, train_y_cv)\n",
    "        eval_y_pred = model.predict(eval_x_cv)\n",
    "        F1_cv = f1_score(eval_y_cv, eval_y_pred)\n",
    "        F1_sum_cv += F1_cv\n",
    "\n",
    "    # get average F1 score\n",
    "    F1_avg_train_score = F1_sum_cv / 5\n",
    "\n",
    "    # test\n",
    "    with ignorePrint():\n",
    "        model.fit(x_train_bin, y_train)\n",
    "    test_y_pred = model.predict(x_test_bin)\n",
    "    # get accuracy and F1 score for test dataset\n",
    "    accuracy_test_score = accuracy_score(y_test, test_y_pred)\n",
    "    F1_test_score = f1_score(y_test, test_y_pred)\n",
    "    \n",
    "    F1_avg_train_scores.append(F1_avg_train_score)\n",
    "    accuracy_test_scores.append(accuracy_test_score)\n",
    "    F1_test_scores.append(F1_test_score)\n",
    "    depth_budgets.append(k+1)\n",
    "\n",
    "# plot\n",
    "plt.plot(depth_budgets, F1_avg_train_scores, color='r')\n",
    "plt.plot(depth_budgets, accuracy_test_scores, color='g')\n",
    "plt.plot(depth_budgets, F1_test_scores, color='b')\n",
    "plt.xlabel('Depth Budget')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(['Avg F1 on train data', 'accuracy on test data', 'F1 on test data'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57505e94-2261-465f-8203-2cef622507b5",
   "metadata": {},
   "source": [
    "## (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db11f94-f860-4543-946b-9f64be7209c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for traditional decision tree in (b)\n",
    "# using the best parameters obtained in (b)\n",
    "model = DecisionTreeClassifier(criterion='gini', max_depth=6, splitter='best', random_state=42)\n",
    "model.fit(x_train, y_train)\n",
    "test_y_pred_trad = model.predict(x_test)\n",
    "# get accuracy and F1 score for test dataset\n",
    "accuracy_test_score_trad = accuracy_score(y_test, test_y_pred_trad)\n",
    "F1_test_score_trad = f1_score(y_test, test_y_pred_trad)\n",
    "print('traditional decision tree: accuracy_test =', accuracy_test_score_trad, ', F1_test =', F1_test_score_trad)\n",
    "\n",
    "# for GOSDT\n",
    "# using Depth Budget = 6\n",
    "model = GOSDT({\n",
    "        'regularization': 0.02,\n",
    "        'depth_budget': 6,\n",
    "        'similar_support': False\n",
    "})\n",
    "with ignorePrint():\n",
    "    model.fit(x_train_bin, y_train)\n",
    "test_y_pred_gosdt = model.predict(x_test_bin)\n",
    "# get accuracy and F1 score for test dataset\n",
    "accuracy_test_score_gosdt = accuracy_score(y_test, test_y_pred_gosdt)\n",
    "F1_test_score_gosdt = f1_score(y_test, test_y_pred_gosdt)\n",
    "print('gosdt: accuracy_test =', accuracy_test_score_gosdt, ', F1_test =', F1_test_score_gosdt)\n",
    "# Therefore, gosdt has better performance than traditional decision tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
